# Unlocking the Power of Agentic Coding: Self-Assessment Rubric

**Purpose:** This rubric helps you identify where you are today so you can start learning from the right place. There are no wrong answers - honest self-placement means you spend time on what actually helps you, not what you already know.

Read each track description below. Pick the one that best matches where you are *right now*, not where you want to be.

---

## Track 1 - Building the Mental Model

**This is you if:**

- [ ] You haven't used AI coding tools, or you've only used ChatGPT / Claude chat for general questions
- [ ] You're not sure what "agentic coding" means or how it differs from asking an AI a question
- [ ] You've heard of Copilot or similar tools but haven't used them regularly
- [ ] You're skeptical about AI coding tools or unsure where they actually help

**What you'll focus on:**

- What LLMs are good at and where they fall short
- The spectrum from chat to autocomplete to agentic workflows
- How to give AI useful context about your code
- Building trust through verification - how to review AI-generated code critically

**After this track, you'll be able to:** Have a clear mental model for when and why to reach for AI tools, and use them for straightforward tasks with confidence.

---

## Track 2 - From Autocomplete to Delegation

**This is you if:**

- [ ] You use GitHub Copilot (or similar) regularly for autocomplete and inline suggestions
- [ ] You sometimes paste code into ChatGPT or Claude to ask questions or get help
- [ ] You're comfortable with AI suggesting lines of code, but you haven't given an AI a multi-step task and let it work autonomously
- [ ] You occasionally accept AI suggestions without fully understanding them (no judgment - this is common)

**What you'll focus on:**

- The fundamental shift from "AI completes my line" to "AI executes my intent"
- How to break a problem down so an agent can work on it effectively
- Providing specification and context - the skill that separates good from great agentic workflows
- Reading and reviewing agentic output (diffs, multi-file changes) efficiently

**After this track, you'll be able to:** Use agentic tools like Claude Code to complete real multi-step tasks - adding features, fixing bugs, refactoring - with appropriate oversight.

---

## Track 3 - Sharpening the Edge

**This is you if:**

- [ ] You've used agentic coding tools (Claude Code, Cursor Agent, Aider, etc.) for real work
- [ ] You understand the basics of giving an agent context and reviewing its output
- [ ] You sometimes get great results and sometimes get frustrated - and you're not always sure why
- [ ] You want to develop consistent, repeatable workflows rather than relying on trial and error

**What you'll focus on:**

- When to plan vs. when to just let the agent go
- Effective patterns: iterative refinement, test-driven agentic workflows, using CLAUDE.md and project context files
- Knowing when to stop the agent and course-correct vs. letting it finish
- Integrating agentic coding into your daily workflow without slowing down

**After this track, you'll be able to:** Consistently get high-quality results from agentic tools, know your own workflow patterns, and help teammates level up.

---

## How to Use Your Result

1. **Pick your track.** If you're between two, start with the lower one. Skipping foundations creates gaps that slow you down later.
2. **Work through your track's exercises** (coming soon) using our actual codebase.
3. **Move to the next track when you're ready.** There's no timeline - some people move through Track 1 in an afternoon, others take a week. Both are fine.
4. **Share what you learn.** Drop tips, surprises, and "aha" moments in the team channel as you go. Teaching reinforces learning.

---

## Quick Placement Guide

Still not sure? Answer these two questions:

**1. Have you used an AI tool to write code (not just answer questions)?**
- No -> **Track 1**
- Yes, autocomplete/inline suggestions -> **Track 2**
- Yes, multi-step agentic tasks -> **Track 3**

**2. Could you explain to a teammate how to give an AI agent enough context to complete a task in our codebase?**
- No, or I'm not sure what that means -> **Track 1 or 2**
- I have a general idea but I'm inconsistent -> **Track 2 or 3**
- Yes, and I do it regularly -> **Track 3**
